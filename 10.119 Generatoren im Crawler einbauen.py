"""
Statt einer Liste, f√ºgen wir nun einen Generator ein, indem wir die Listengenerierung (articles = []) entfernen und
auch den gecrawlten Inhalt (CrawledArticle) nicht mehr per append an die Liste anh√§ngen,sondern die Funktion per
Yield-Befehl stoppen

"""

import requests  # Importiert das 'requests'-Modul, um HTTP-Anfragen (z.B. Webseiten abrufen) durchzuf√ºhren.
from bs4 import BeautifulSoup  # Importiert 'BeautifulSoup' aus dem 'bs4'-Modul, um HTML- und XML-Dokumente zu parsen.
from urllib.parse import \
    urljoin  # Importiert 'urljoin' f√ºr das Zusammenf√ºgen von Basis-URLs und relativen Pfaden zu absoluten URLs.
import time  # Importiert das 'time'-Modul, um Zeitverz√∂gerungen zu steuern (gut, um Server nicht zu √ºberlasten).


# %%
class CrawledArticle():
    # Diese Klasse dient als einfacher Datencontainer f√ºr die Informationen eines jeden Artikels.
    def __init__(self, title, emoji, content, image):
        self.title = title  # Der Titel des Artikels.
        self.emoji = emoji  # Das Emoji, das den Artikel begleitet.
        self.content = content  # Der Textinhalt des Artikels.
        self.image = image  # Die absolute URL des Artikelbildes.

    def __repr__(self):
        # Diese spezielle Methode definiert, wie ein CrawledArticle-Objekt ausgegeben wird,
        # wenn es direkt gedruckt oder in einer Liste angezeigt wird.
        return f"CrawledArticle(Titel: '{self.title}', Emoji: '{self.emoji}')"


class ArticleFetcher():
    # Diese Klasse ist f√ºr das Abrufen und Parsen der Artikel von der Webseite zust√§ndig,
    # einschlie√ülich der Navigation √ºber mehrere Seiten hinweg.
    def fetch(self):
        url = "http://python.beispiel.programmierenlernen.io/index.php"  # **Start-URL f√ºr den Crawler.**

        # **Wichtige √Ñnderung: Die Methode wird zu einem Generator**
        # Statt eine Liste aller Artikel zu erstellen und am Ende zur√ºckzugeben,
        # verwenden wir 'yield'. Das macht 'fetch' zu einem Generator.
        # Ein Generator gibt die Artikel einzeln zur√ºck, sobald sie gefunden werden,
        # was bei gro√üen Datenmengen sehr speichereffizient ist.

        # **Schleife f√ºr die Paginierung:**
        # Die 'while'-Schleife l√§uft so lange, wie 'url' einen g√ºltigen Wert hat.
        # Sie stoppt, wenn keine "N√§chste Seite"-Schaltfl√§che mehr gefunden wird und 'url' auf "" gesetzt wird.
        while url != "":
            print(f"Besuche URL: {url}")  # Zur Debugging-Zwecken: Zeigt die aktuell besuchte URL an.
            time.sleep(1)  # **Kurze Pause von 1 Sekunde:** Wichtig, um den Server nicht zu √ºberlasten
            # und das Risiko einer Blockierung zu minimieren.

            r = requests.get(url)  # Sendet eine HTTP GET-Anfrage an die aktuelle URL, um den HTML-Inhalt zu holen.
            doc = BeautifulSoup(r.text, "html.parser")  # Parsen des HTML-Inhalts der Antwort mit BeautifulSoup
            # f√ºr eine einfache Navigation und Datenextraktion.

            # Iteriert √ºber jedes HTML-Element mit der CSS-Klasse "card" auf der aktuellen Seite.
            # Jede "card" repr√§sentiert einen einzelnen Artikel auf der Webseite.
            for card in doc.select(".card"):
                emoji = card.select_one(".emoji").text  # Extrahiert das Emoji aus dem Element mit der Klasse "emoji".
                content = card.select_one(".card-text").text  # Extrahiert den Haupttext des Artikels.
                # Sucht das zweite <span>-Element innerhalb eines Elements mit der Klasse "card-title"
                # und extrahiert dessen Text als Titel.
                title = card.select(".card-title span")[1].text
                # Extrahiert die relative Bild-URL aus dem 'src'-Attribut des <img>-Tags
                # und wandelt sie dann mit urljoin in eine vollst√§ndige, absolute URL um.
                image = urljoin(url, card.select_one("img").attrs["src"])

                # **Der 'yield'-Befehl:**
                # Gibt ein CrawledArticle-Objekt zur√ºck, ohne die Funktion zu beenden.
                # Die Ausf√ºhrung der Funktion wird bis zum n√§chsten 'next()' oder zur n√§chsten Iteration pausiert.
                yield CrawledArticle(title, emoji, content, image)

            # **Paginierungslogik: Suche nach dem "Zur n√§chsten Seite"-Button**
            # Sucht nach dem HTML-Element, das den "Zur n√§chsten Seite"-Button darstellt.
            # Dieser wird typischerweise innerhalb eines Bereichs mit der Klasse ".navigation"
            # und selbst mit der Klasse ".btn" identifiziert.
            next_button = doc.select_one(".navigation .btn")

            # √úberpr√ºft, ob ein "N√§chste Seite"-Button auf der aktuellen Seite gefunden wurde.
            if next_button:
                # Wenn ja, hole den Wert des 'href'-Attributs, der die URL der n√§chsten Seite enth√§lt.
                next_href = next_button.attrs["href"]
                # Wandelt den relativen 'href'-Pfad in eine absolute URL um,
                # indem die aktuelle 'url' als Basis f√ºr die Verkn√ºpfung verwendet wird.
                next_href = urljoin(url, next_href)
                # Setzt die 'url' f√ºr den n√§chsten Schleifendurchlauf auf die neu gefundene URL der n√§chsten Seite.
                url = next_href
            else:
                # Wenn kein "N√§chste Seite"-Button gefunden wurde (d.h., wir sind auf der letzten Seite),
                # wird 'url' auf einen leeren String ("") gesetzt. Dies beendet die 'while'-Schleife
                # beim n√§chsten Pr√ºfen der Schleifenbedingung (url != "").
                url = ""

        # **Kein 'return articles' hier:**
        # Da 'fetch' ein Generator ist und die Artikel √ºber 'yield' zur√ºckgibt,
        # ist kein explizites 'return' Statement am Ende der Funktion n√∂tig.
        # Der Generator beendet seine Ausf√ºhrung automatisch, wenn die Schleife abgeschlossen ist.


# --- Ausf√ºhrung des Crawlers ---
fetcher = ArticleFetcher()  # Erstellt eine Instanz der ArticleFetcher-Klasse.
# **Aufruf des Generators:**
# 'fetcher.fetch()' gibt nun ein Generator-Objekt zur√ºck, das Artikel liefert,
# sobald sie vom Crawler gefunden werden. 'all_articles' ist dieses Generator-Objekt.
all_articles = fetcher.fetch()

# --- Ausgabe der Ergebnisse ---
print("\n--- Alle gesammelten Artikel ---")
# **Iteration √ºber den Generator:**
# Die 'for'-Schleife iteriert √ºber das Generator-Objekt 'all_articles'.
# Bei jedem Schleifendurchlauf wird der n√§chste verf√ºgbare Artikel vom Generator angefordert,
# bis der Generator keine weiteren Artikel mehr liefert (d.h., alle Seiten wurden durchsucht).
for article in all_articles:
    print(f"{article.emoji}: {article.title}")

"""Ausgabe:
Besuche URL: http://python.beispiel.programmierenlernen.io/index.php
üò©: Polarised modular conglomeration
üòê: Cross-group contextually-based middleware
üòå: De-engineered encompassing structure
üòö: Fully-configurable multi-tasking interface
üò†: Versatile eco-centric core
üòÆ: Optional maximized utilisation
üò¢: Open-architected secondary product
Besuche URL: http://python.beispiel.programmierenlernen.io/index.php?page=2
üò†: Realigned zerotolerance function
üòÜ: Quality-focused user-facing help-desk
üò§: Proactive user-facing opensystem
üòü: Decentralized holistic moderator
üòå: Mandatory tangible application
üòì: Digitized dedicated budgetarymanagement
üòû: Organized well-modulated concept
usw.
"""



"""
Absolut! Gerne beschreibe ich die Reihenfolge des Codes und was wann passiert, Schritt f√ºr Schritt.
Ablauf des Web-Crawlers: Schritt f√ºr Schritt erkl√§rt

Dein Code ist darauf ausgelegt, Artikel von einer Webseite zu sammeln, die √ºber mehrere Seiten hinweg paginiert ist. Hier ist die genaue Reihenfolge, wie der Code ausgef√ºhrt wird:
1. Vorbereitung und Definitionen (Start des Skripts)

    Importe: Zuerst werden die ben√∂tigten Bibliotheken geladen:
        requests f√ºr das Senden von HTTP-Anfragen (Webseiten herunterladen).
        BeautifulSoup zum Parsen des HTML-Inhalts und Extrahieren von Daten.
        urljoin zum Zusammenf√ºgen von Basis-URLs und relativen Pfaden.
        time f√ºr das Einf√ºgen von Pausen.
    Klassen-Definitionen:
        Die Klasse CrawledArticle wird als Bauplan f√ºr jedes Artikelobjekt definiert. Sie legt fest, welche Informationen (Titel, Emoji, Inhalt, Bild-URL) ein gescrappter Artikel haben wird und wie er sp√§ter lesbar ausgegeben wird (__repr__-Methode).
        Die Klasse ArticleFetcher wird definiert. Sie enth√§lt die gesamte Logik f√ºr das Crawling ‚Äì also das Navigieren zwischen den Seiten und das Extrahieren der Daten.

2. Initialisierung des Crawling-Prozesses (Am Ende des Skripts)

    Instanziierung: Ganz am Ende deines Skripts, au√üerhalb der Klassen, wird eine Instanz (ein Objekt) der ArticleFetcher-Klasse erstellt: fetcher = ArticleFetcher(). Dein Crawler ist nun bereit.
    Aufruf der fetch-Methode: Die fetch()-Methode dieses fetcher-Objekts wird aufgerufen: all_articles = fetcher.fetch(). Hier beginnt die eigentliche Arbeit des Crawlers.

3. Start der Paginierungs-Schleife (Innerhalb der fetch()-Methode)

    Start-URL: Im Inneren der fetch()-Methode wird die Variable url mit der initialen URL der Webseite belegt (der ersten Seite).
    Die while-Schleife: Eine while-Schleife beginnt: while url != "". Diese Schleife ist das Herzst√ºck der Paginierungslogik. Sie wird so lange ausgef√ºhrt, wie die url-Variable einen Wert hat (also eine Seite zum Besuchen existiert). Sobald keine n√§chste Seite mehr gefunden wird und url auf einen leeren String gesetzt wird, endet die Schleife.

4. Verarbeitung einer einzelnen Seite (Pro Durchlauf der while-Schleife)

    Statusmeldung & Pause: Bei jedem Schleifendurchlauf wird die aktuell besuchte URL ausgegeben (print(f"Besuche URL: {url}")). Direkt danach wird eine Pause von 1 Sekunde (time.sleep(1)) eingelegt. Das ist entscheidend, um den Webserver nicht zu √ºberlasten und einer m√∂glichen Blockierung vorzubeugen.
    Webseiten-Anfrage: requests.get(url) sendet eine HTTP-GET-Anfrage an die aktuelle URL, um den vollst√§ndigen HTML-Inhalt der Seite herunterzuladen.
    HTML Parsen: Der heruntergeladene HTML-Text (r.text) wird an BeautifulSoup √ºbergeben (doc = BeautifulSoup(r.text, "html.parser")). Dadurch wird der HTML-Text in ein Python-Objekt umgewandelt, das leicht durchsucht und manipuliert werden kann.

5. Extrahieren der Artikel auf der aktuellen Seite

    Artikel-Iteration: Eine for-Schleife (for card in doc.select(".card"):) durchl√§uft alle HTML-Elemente auf der aktuellen Seite, die die CSS-Klasse "card" besitzen. Jedes dieser Elemente wird als Container f√ºr einen Artikel betrachtet.
    Daten-Extraktion: Innerhalb dieser for-Schleife werden f√ºr jede "card" die spezifischen Informationen des Artikels extrahiert:
        Das Emoji (.emoji-Klasse).
        Der Inhalt (.card-text-Klasse).
        Der Titel (das zweite <span>-Element innerhalb von .card-title).
        Die Bild-URL: Hier wird der Wert des src-Attributs des <img>-Tags geholt. Ganz wichtig: urljoin(url, ...) kombiniert diese oft relative Bild-URL mit der aktuellen Basis-URL der Seite, um eine vollst√§ndige, absolute URL zu erzeugen, die sp√§ter direkt aufrufbar ist.
    Artikel "Yielding": Statt die gesammelten Daten in eine Liste einzuf√ºgen, wird yield CrawledArticle(...) verwendet. Das bedeutet, dass ein CrawledArticle-Objekt sofort zur√ºckgegeben wird, ohne die fetch-Methode zu verlassen. Die Methode pausiert und wartet auf den n√§chsten Abruf.

6. Paginierungslogik: Suche nach der n√§chsten Seite

    "N√§chste Seite"-Button suchen: Nachdem alle Artikel der aktuellen Seite √ºber yield bereitgestellt wurden, sucht der Code nach dem HTML-Element, das den "Zur n√§chsten Seite"-Button repr√§sentiert (next_button = doc.select_one(".navigation .btn")).
    Bedingte Logik (if/else):
        Wenn der Button gefunden wird (if next_button ist True):
            Der Wert des href-Attributs (der Link zur n√§chsten Seite) wird aus dem Button-Element ausgelesen (next_href = next_button.attrs["href"]).
            Auch dieser relative Pfad wird mit urljoin(url, next_href) in eine absolute URL umgewandelt.
            Die url-Variable wird auf diese neu gefundene URL der n√§chsten Seite gesetzt (url = next_href). Dies sorgt daf√ºr, dass die while-Schleife im n√§chsten Durchlauf diese neue Seite l√§dt und verarbeitet.
        Wenn der Button NICHT gefunden wird (else-Zweig):
            Das bedeutet, der Crawler hat die letzte Seite erreicht.
            Die url-Variable wird auf einen leeren String (url = "") gesetzt. Dadurch wird die Bedingung der while-Schleife (url != "") beim n√§chsten Pr√ºfen False, und die Schleife beendet sich.

7. Abschluss und Ergebnisausgabe

    Generator-Ende: Sobald die while-Schleife in der fetch()-Methode beendet ist (weil url ein leerer String ist), signalisiert der Generator, dass keine weiteren Werte mehr geliefert werden. Die fetch()-Methode wird ohne ein explizites return beendet.
    Iteration √ºber den Generator: Au√üerhalb der Klasse, im Hauptteil des Skripts, wo all_articles = fetcher.fetch() aufgerufen wurde, wird nun √ºber das zur√ºckgegebene Generator-Objekt iteriert: for article in all_articles:.
    Ausgabe: Bei jedem Durchlauf dieser for-Schleife wird der n√§chste verf√ºgbare Artikel vom Generator angefordert und dessen Emoji und Titel auf der Konsole ausgegeben. Dies geschieht, bis der Generator keine Artikel mehr liefert.

Dieser schrittweise Ablauf erm√∂glicht es deinem Crawler, systematisch alle Seiten der Webseite zu besuchen, die relevanten Daten zu extrahieren und sie dir effizient zur Verf√ºgung zu stellen.
------------------------

√Ñnderungen von der Listen-basierten zur Generator-basierten Version

Die Kern√§nderungen, um von einer Listen-basierten Datensammlung zu einem Generator zu wechseln und die Paginierung zu implementieren, betreffen haupts√§chlich die fetch-Methode in der ArticleFetcher-Klasse.

Hier sind die spezifischen Zeilen, die entweder entfernt oder ge√§ndert wurden:
1. Initialisierung der Artikelliste (Entfernt)

In der alten Version wurde eine leere Liste namens articles initialisiert, um alle gesammelten CrawledArticle-Objekte aufzunehmen:
Python

# --- DIESE ZEILE WURDE ENTFERNT ---
articles = []  # Initialisiert eine leere Liste, in der alle extrahierten 'CrawledArticle'-Objekte gespeichert werden.

Grund f√ºr die Entfernung: Mit einem Generator brauchen wir diese Liste nicht mehr. Der Generator "gibt" die Artikel einzeln zur√ºck, sobald sie gefunden werden, ohne sie alle im Speicher sammeln zu m√ºssen.
2. Hinzuf√ºgen zur Artikelliste (Ge√§ndert zu yield)

Anstatt die gesammelten Artikel der articles-Liste hinzuzuf√ºgen, wurde der Aufruf an CrawledArticle so ge√§ndert, dass er jetzt mit dem Schl√ºsselwort yield verwendet wird.

Alte Zeile:
Python

# --- DIESE ZEILE WURDE GE√ÑNDERT ---
# crawled = CrawledArticle(title, emoji, content, image)  # Erstellt ein neues 'CrawledArticle'-Objekt mit den extrahierten und bereinigten Daten.
# articles.append(crawled)  # F√ºgt das neu erstellte Artikel-Objekt der 'articles'-Liste hinzu.

Neue Zeile:
Python

# --- GE√ÑNDERT ZU DIESER ZEILE ---
yield CrawledArticle(title, emoji, content, image)  # Gibt das Artikel-Objekt als Teil des Generators zur√ºck.

Grund f√ºr die √Ñnderung: yield macht die fetch-Methode zu einem Generator. Es gibt das erzeugte Objekt zur√ºck und pausiert die Funktion, anstatt sie zu beenden. Beim n√§chsten Aufruf wird die Ausf√ºhrung an der Stelle fortgesetzt, wo sie aufgeh√∂rt hat.
3. R√ºckgabe der Artikelliste (Entfernt)

In der alten Version wurde am Ende der fetch-Methode die gesammelte Liste aller Artikel zur√ºckgegeben:
Python

# --- DIESE ZEILE WURDE ENTFERNT ---
# return articles  # Gibt die Liste aller gescrapten Artikel-Objekte zur√ºck.

Grund f√ºr die Entfernung: Da die fetch-Methode jetzt ein Generator ist, gibt sie keine explizite Liste mehr zur√ºck. Die Artikel werden einzeln √ºber yield geliefert. Ein Generator beendet seine Ausf√ºhrung implizit, wenn er keine weiteren yield-Anweisungen mehr erreicht (in deinem Fall, wenn die while-Schleife endet). Das Beibehalten dieser Zeile war auch die Ursache f√ºr den NameError, den du zuvor hattest.

Zus√§tzlich zu diesen √Ñnderungen wurde die while-Schleife f√ºr die Paginierung und die time.sleep(1)-Pause hinzugef√ºgt, um die Funktionalit√§t des Multi-Seiten-Crawlings zu erm√∂glichen, die in der urspr√ºnglichen Version nicht vorhanden war.

Diese Anpassungen machen deinen Crawler nicht nur f√§hig, mehrere Seiten zu durchsuchen, sondern auch deutlich speichereffizienter durch die Nutzung des Generator-Prinzips.


"""
